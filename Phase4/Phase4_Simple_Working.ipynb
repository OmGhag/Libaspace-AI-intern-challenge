{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ML Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 4: SIMPLE IMPLEMENTATION\n",
      "======================================================================\n",
      "\n",
      " Loaded 23 fields from Phase 3\n",
      "\n",
      "Phase 3 Summary:\n",
      "  Accuracy: 87.0%\n",
      "  Correct: 20/23\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PHASE 4: SIMPLE IMPLEMENTATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Phase 3 results\n",
    "try:\n",
    "    phase3_results = pd.read_csv(r'C:\\Users\\omgha\\OneDrive\\Documents\\GitHub\\Libaspace-AI-intern-challenge\\Phase3\\logistic_regression_results.csv')\n",
    "    print(f\"\\n Loaded {len(phase3_results)} fields from Phase 3\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Could not find phase3_with_logistic_regression.csv\")\n",
    "    print(\"   Make sure you saved Phase 3 results to CSV\")\n",
    "    phase3_results = None\n",
    "\n",
    "if phase3_results is not None:\n",
    "    print(f\"\\nPhase 3 Summary:\")\n",
    "    accuracy = (phase3_results['lr_correct'].mean())\n",
    "    print(f\"  Accuracy: {accuracy*100:.1f}%\")\n",
    "    print(f\"  Correct: {phase3_results['lr_correct'].sum()}/23\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Confidence Levels (Does ML Need Help?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CONFIDENCE ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Confidence Statistics:\n",
      "  Min: 0.545\n",
      "  Max: 0.995\n",
      "  Mean: 0.824\n",
      "  Median: 0.853\n",
      "\n",
      "üîç Low-Confidence Cases (< 0.85): 10\n",
      "\n",
      "These fields need Claude's help:\n",
      " first_name           | Conf: 0.807\n",
      " last_name            | Conf: 0.817\n",
      " question_7968643005  | Conf: 0.762\n",
      " question_7968646005  | Conf: 0.826\n",
      " question_7968647005  | Conf: 0.738\n",
      " question_7968651005  | Conf: 0.624\n",
      " question_7968652005  | Conf: 0.593\n",
      " question_7968655005  | Conf: 0.545\n",
      " question_7968656005  | Conf: 0.609\n",
      " 4014112005           | Conf: 0.845\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONFIDENCE ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nConfidence Statistics:\")\n",
    "    print(f\"  Min: {phase3_results['lr_confidence'].min():.3f}\")\n",
    "    print(f\"  Max: {phase3_results['lr_confidence'].max():.3f}\")\n",
    "    print(f\"  Mean: {phase3_results['lr_confidence'].mean():.3f}\")\n",
    "    print(f\"  Median: {phase3_results['lr_confidence'].median():.3f}\")\n",
    "    \n",
    "    # Find low-confidence cases\n",
    "    LOW_CONF_THRESHOLD = 0.85\n",
    "    uncertain_fields = phase3_results[\n",
    "        phase3_results['lr_confidence'] < LOW_CONF_THRESHOLD\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nüîç Low-Confidence Cases (< {LOW_CONF_THRESHOLD}): {len(uncertain_fields)}\")\n",
    "    \n",
    "    if len(uncertain_fields) == 0:\n",
    "        print(f\"\\nALL PREDICTIONS ARE CONFIDENT!\")\n",
    "        print(f\"   ‚Üí Phase 3 is already good\")\n",
    "        print(f\"   ‚Üí No LLM needed!\")\n",
    "        print(f\"   ‚Üí Just submit Phase 3 results\")\n",
    "    else:\n",
    "        print(f\"\\nThese fields need Claude's help:\")\n",
    "        for idx, field in uncertain_fields.iterrows():\n",
    "            print(f\" {field['field_id']:20} | Conf: {field['lr_confidence']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create Simple Prompts (NOT Confusing Few-Shot Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SIMPLE PROMPTS FOR 10 UNCERTAIN FIELDS\n",
      "======================================================================\n",
      "\n",
      "Instructions:\n",
      "1. Go to https://claude.ai\n",
      "2. Copy EACH prompt below\n",
      "3. Paste into Claude\n",
      "4. Record Claude's one-word answer (TEXT or SELECT)\n",
      "5. Come back and update Step 4 with responses\n",
      "\n",
      "======================================================================\n",
      "PROMPT 1: first_name\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: first_name\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.81)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 2: last_name\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: last_name\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.82)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 3: question_7968643005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968643005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.76)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 4: question_7968646005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968646005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.83)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 5: question_7968647005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968647005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.74)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 6: question_7968651005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968651005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.62)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 7: question_7968652005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968652005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.59)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 8: question_7968655005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968655005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.54)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 9: question_7968656005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: question_7968656005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.61)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n",
      "\n",
      "======================================================================\n",
      "PROMPT 10: 4014112005\n",
      "======================================================================\n",
      "Classify this form field as TEXT input or SELECT dropdown.\n",
      "\n",
      "Field Label: 4014112005\n",
      "DOM: tag:input, opts:0, yes_no:0\n",
      "\n",
      "Our ML model predicted: text (confidence: 0.84)\n",
      "\n",
      "Based on this field information, is this:\n",
      "(A) TEXT input - user types a response\n",
      "(B) SELECT dropdown - user picks from options\n",
      "\n",
      "Respond with ONE WORD: TEXT or SELECT\n",
      "\n",
      "‚Üí Copy above, paste into claude.ai, record response\n"
     ]
    }
   ],
   "source": [
    "def create_simple_prompt(field_data):\n",
    "    \"\"\"\n",
    "    Create a SIMPLE, CLEAR prompt for Claude\n",
    "    NO confusing few-shot examples\n",
    "    NO contradictory patterns\n",
    "    \n",
    "    Just: Field info + ask for classification\n",
    "    \"\"\"\n",
    "    \n",
    "    label = field_data.get('field_id', 'Unknown')\n",
    "    options_count = int(field_data.get('options_count', 0))\n",
    "    has_options = int(field_data.get('has_options', 0))\n",
    "    is_yes_no = int(field_data.get('is_yes_no_question', 0))\n",
    "    predicted = field_data.get('lr_prediction', 'unknown')\n",
    "    confidence = float(field_data.get('lr_confidence', 0))\n",
    "    \n",
    "    # DOM snapshot\n",
    "    dom = f\"tag:{'select' if has_options else 'input'}, opts:{options_count}, yes_no:{is_yes_no}\"\n",
    "    \n",
    "    # Simple, direct prompt - NO EXAMPLES\n",
    "    prompt = f\"\"\"Classify this form field as TEXT input or SELECT dropdown.\n",
    "\n",
    "Field Label: {label}\n",
    "DOM: {dom}\n",
    "\n",
    "Our ML model predicted: {predicted} (confidence: {confidence:.2f})\n",
    "\n",
    "Based on this field information, is this:\n",
    "(A) TEXT input - user types a response\n",
    "(B) SELECT dropdown - user picks from options\n",
    "\n",
    "Respond with ONE WORD: TEXT or SELECT\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "if phase3_results is not None and len(uncertain_fields) > 0:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"SIMPLE PROMPTS FOR {len(uncertain_fields)} UNCERTAIN FIELDS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    print(f\"\\nInstructions:\")\n",
    "    print(f\"1. Go to https://claude.ai\")\n",
    "    print(f\"2. Copy EACH prompt below\")\n",
    "    print(f\"3. Paste into Claude\")\n",
    "    print(f\"4. Record Claude's one-word answer (TEXT or SELECT)\")\n",
    "    print(f\"5. Come back and update Step 4 with responses\")\n",
    "    \n",
    "    # Show each prompt\n",
    "    for i, (idx, field) in enumerate(uncertain_fields.iterrows(), 1):\n",
    "        prompt = create_simple_prompt(field.to_dict())\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PROMPT {i}: {field['field_id']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(prompt)\n",
    "        print(f\"\\n‚Üí Copy above, paste into claude.ai, record response\")\n",
    "else:\n",
    "    if phase3_results is not None:\n",
    "        print(\"\\n‚úÖ All fields have high confidence - no prompts needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record Claude's Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: RECORD CLAUDE'S RESPONSES\n",
      "======================================================================\n",
      "\n",
      "Recorded 10 responses from Claude\n",
      "  first_name: TEXT\n",
      "  last_name: TEXT\n",
      "  question_7968643005: TEXT\n",
      "  question_7968646005: TEXT\n",
      "  question_7968647005: TEXT\n",
      "  question_7968651005: TEXT\n",
      "  question_7968652005: TEXT\n",
      "  question_7968655005: TEXT\n",
      "  question_7968656005: TEXT\n",
      "  4014112005: TEXT\n"
     ]
    }
   ],
   "source": [
    "# After getting Claude's responses from claude.ai, fill this dictionary\n",
    "# Format: 'field_id': 'text' or 'select'\n",
    "\n",
    "llm_responses = {\n",
    "    'first_name': 'text',\n",
    "    'last_name': 'text',\n",
    "    'question_7968643005': 'text',\n",
    "    'question_7968646005': 'text',\n",
    "    'question_7968647005': 'text',\n",
    "    'question_7968651005':'text',\n",
    "    'question_7968652005': 'text',\n",
    "    'question_7968655005': 'text',\n",
    "    'question_7968656005': 'text',\n",
    "    '4014112005': 'text'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: RECORD CLAUDE'S RESPONSES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(llm_responses) == 0:\n",
    "    print(f\"\\nWaiting for Claude's responses...\")\n",
    "    print(f\"\\nWhat to do:\")\n",
    "    print(f\"1. Go to https://claude.ai\")\n",
    "    print(f\"2. For EACH prompt above, ask Claude\")\n",
    "    print(f\"3. Record Claude's answer (TEXT or SELECT)\")\n",
    "    print(f\"4. Update the 'llm_responses' dictionary above\")\n",
    "    print(f\"5. Run this cell again\")\n",
    "else:\n",
    "    print(f\"\\nRecorded {len(llm_responses)} responses from Claude\")\n",
    "    for field_id, response in llm_responses.items():\n",
    "        print(f\"  {field_id}: {response.upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Claude's Responses to Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "APPLYING CLAUDE'S RESPONSES\n",
      "======================================================================\n",
      "\n",
      "Applying 10 Claude responses...\n",
      "  Updated first_name: Claude said TEXT\n",
      "  Updated last_name: Claude said TEXT\n",
      "  Updated question_7968643005: Claude said TEXT\n",
      "  Updated question_7968646005: Claude said TEXT\n",
      "  Updated question_7968647005: Claude said TEXT\n",
      "  Updated question_7968651005: Claude said TEXT\n",
      "  Updated question_7968652005: Claude said TEXT\n",
      "  Updated question_7968655005: Claude said TEXT\n",
      "  Updated question_7968656005: Claude said TEXT\n",
      "  Updated 4014112005: Claude said TEXT\n",
      "\n",
      "Applied all responses\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    # Copy Phase 3 results\n",
    "    final_results = phase3_results.copy()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"APPLYING CLAUDE'S RESPONSES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if len(llm_responses) == 0:\n",
    "        # No Claude responses yet, use Phase 3 as-is\n",
    "        print(f\"\\n No Claude responses yet\")\n",
    "        print(f\"   Using Phase 3 results as final predictions\")\n",
    "        \n",
    "        final_results['final_prediction'] = final_results['lr_prediction']\n",
    "        final_results['final_confidence'] = final_results['lr_confidence']\n",
    "        final_results['decision_source'] = 'phase3'\n",
    "    else:\n",
    "        # Apply Claude's responses\n",
    "        print(f\"\\nApplying {len(llm_responses)} Claude responses...\")\n",
    "        \n",
    "        final_results['final_prediction'] = final_results['lr_prediction'].copy()\n",
    "        final_results['final_confidence'] = final_results['lr_confidence'].copy()\n",
    "        final_results['decision_source'] = 'phase3'\n",
    "        \n",
    "        # Update with Claude's answers\n",
    "        for field_id, claude_pred in llm_responses.items():\n",
    "            mask = final_results['field_id'] == field_id\n",
    "            if mask.sum() > 0:\n",
    "                # Use Claude's prediction\n",
    "                final_results.loc[mask, 'final_prediction'] = claude_pred.lower()\n",
    "                final_results.loc[mask, 'final_confidence'] = 0.90  # Trust Claude's answer\n",
    "                final_results.loc[mask, 'decision_source'] = 'claude'\n",
    "                print(f\"  Updated {field_id}: Claude said {claude_pred.upper()}\")\n",
    "    \n",
    "    # Mark which are correct\n",
    "    final_results['final_correct'] = final_results['final_prediction'] == final_results['true_kind']\n",
    "    \n",
    "    print(f\"\\nApplied all responses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Calculate Final Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "\n",
      "ACCURACY PROGRESSION:\n",
      "  Phase 1 (Baseline):              73.9% (17/23)\n",
      "  Phase 2 (Rules):                 87.0% (20/23)\n",
      "  Phase 3 (Logistic Regression):     87.0% (20/23)\n",
      "  Phase 4 (Claude Help):             87.0% (20/23)\n",
      "\n",
      "‚Üí No change from Phase 3 (still good!)\n",
      "\n",
      "Saved: phase4_final_results.csv\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    final_accuracy = final_results['final_correct'].mean()\n",
    "    final_correct = final_results['final_correct'].sum()\n",
    "    \n",
    "    # Compare to Phase 3\n",
    "    phase3_accuracy = (phase3_results['lr_correct']).mean()\n",
    "    phase3_correct = phase3_results['lr_correct'].sum()\n",
    "    \n",
    "    print(f\"\\nACCURACY PROGRESSION:\")\n",
    "    print(f\"  Phase 1 (Baseline):              73.9% (17/23)\")\n",
    "    print(f\"  Phase 2 (Rules):                 87.0% (20/23)\")\n",
    "    print(f\"  Phase 3 (Logistic Regression):   {phase3_accuracy*100:6.1f}% ({int(phase3_correct)}/23)\")\n",
    "    print(f\"  Phase 4 (Claude Help):           {final_accuracy*100:6.1f}% ({int(final_correct)}/23)\")\n",
    "    \n",
    "    improvement = (final_accuracy - phase3_accuracy) * 100\n",
    "    if improvement > 0:\n",
    "        print(f\"\\nIMPROVEMENT: +{improvement:.1f}% from Phase 3\")\n",
    "    elif improvement == 0:\n",
    "        print(f\"\\n‚Üí No change from Phase 3 (still good!)\")\n",
    "    else:\n",
    "        print(f\"\\nSlight decrease: {improvement:.1f}%\")\n",
    "    \n",
    "    # Save results\n",
    "    final_results.to_csv('phase4_final_results.csv', index=False)\n",
    "    print(f\"\\nSaved: phase4_final_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM based prompt can be use to classify since they tend to misclassify due to incomplete DOM elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
