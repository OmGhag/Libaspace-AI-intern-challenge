{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Phase 3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL SUBMISSION GENERATOR - CORRECT FORMAT\n",
      "======================================================================\n",
      "\n",
      "Loaded Phase 3 results: 23 fields\n",
      "Loaded dataset: 23 fields\n",
      "\n",
      " Accuracy: 87.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUBMISSION GENERATOR - CORRECT FORMAT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load Phase 3 results\n",
    "try:\n",
    "    phase3_results = pd.read_csv(r'C:\\Users\\omgha\\OneDrive\\Documents\\GitHub\\Libaspace-AI-intern-challenge\\Phase3\\logistic_regression_results.csv')\n",
    "    print(f\"\\nLoaded Phase 3 results: {len(phase3_results)} fields\")\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nCould not find phase3_with_logistic_regression.csv\")\n",
    "    phase3_results = None\n",
    "\n",
    "if phase3_results is not None:\n",
    "    # Load original dataset for labels\n",
    "    try:\n",
    "        dataset = pd.read_csv(r'C:\\Users\\omgha\\OneDrive\\Documents\\GitHub\\Libaspace-AI-intern-challenge\\Phase2\\dataset_with_rules.csv')\n",
    "        print(f\"Loaded dataset: {len(dataset)} fields\")\n",
    "        has_labels = True\n",
    "    except:\n",
    "        print(\"âš ï¸  Could not find dataset for labels\")\n",
    "        dataset = None\n",
    "        has_labels = False\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n Accuracy: {(phase3_results['lr_correct'].mean()*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Evidence for Each Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EVIDENCE GENERATION TEST\n",
      "======================================================================\n",
      "\n",
      "Field: first_name\n",
      "Prediction: text\n",
      "Confidence: 0.807\n",
      "\n",
      "Generated Evidence:\n",
      "  1. No options in raw form\n",
      "  2. Required field\n",
      "  3. Medium confidence (LR â‰¥ 0.70)\n",
      "  4. LR predicts: TEXT\n"
     ]
    }
   ],
   "source": [
    "def generate_evidence(row):\n",
    "    \"\"\"\n",
    "    Generate evidence/reasoning for each prediction\n",
    "    \"\"\"\n",
    "    evidence = []\n",
    "    \n",
    "    # DOM-based evidence\n",
    "    if row['has_options'] == 1:\n",
    "        evidence.append(f\"Options detected ({int(row['options_count'])} options)\")\n",
    "    else:\n",
    "        evidence.append(\"No options in raw form\")\n",
    "    \n",
    "    # Semantic evidence\n",
    "    if row['is_yes_no_question'] == 1:\n",
    "        evidence.append(\"Yes/no question pattern\")\n",
    "    \n",
    "    if row['is_required'] == 1:\n",
    "        evidence.append(\"Required field\")\n",
    "    \n",
    "    # ML confidence evidence\n",
    "    confidence = row['lr_confidence']\n",
    "    if confidence >= 0.95:\n",
    "        evidence.append(\"High confidence (LR â‰¥ 0.95)\")\n",
    "    elif confidence >= 0.85:\n",
    "        evidence.append(\"Medium-high confidence (LR â‰¥ 0.85)\")\n",
    "    elif confidence >= 0.70:\n",
    "        evidence.append(\"Medium confidence (LR â‰¥ 0.70)\")\n",
    "    else:\n",
    "        evidence.append(\"Lower confidence (LR < 0.70)\")\n",
    "    \n",
    "    # Prediction type\n",
    "    pred = str(row['lr_prediction']).lower()\n",
    "    if pred == 'select':\n",
    "        if row['has_options'] == 1:\n",
    "            evidence.append(\"LR predicts: SELECT (based on options)\")\n",
    "        elif row['is_yes_no_question'] == 1:\n",
    "            evidence.append(\"LR predicts: SELECT (despite no visible options)\")\n",
    "        else:\n",
    "            evidence.append(\"LR predicts: SELECT\")\n",
    "    else:\n",
    "        evidence.append(\"LR predicts: TEXT\")\n",
    "    \n",
    "    return evidence\n",
    "\n",
    "if phase3_results is not None:\n",
    "    # Test evidence generation\n",
    "    test_row = phase3_results.iloc[0]\n",
    "    test_evidence = generate_evidence(test_row)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVIDENCE GENERATION TEST\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nField: {test_row['field_id']}\")\n",
    "    print(f\"Prediction: {test_row['lr_prediction']}\")\n",
    "    print(f\"Confidence: {test_row['lr_confidence']:.3f}\")\n",
    "    print(f\"\\nGenerated Evidence:\")\n",
    "    for i, ev in enumerate(test_evidence, 1):\n",
    "        print(f\"  {i}. {ev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING: predictions.jsonl\n",
      "======================================================================\n",
      "\n",
      "âœ… Saved: predictions.jsonl\n",
      "   Lines: 23\n",
      "   Format: One JSON object per line\n",
      "\n",
      "ðŸ“‹ First 3 predictions:\n",
      "\n",
      "1. first_name\n",
      "   Prediction: text\n",
      "   Confidence: 0.81\n",
      "   Evidence: 4 items\n",
      "     - No options in raw form\n",
      "     - Required field\n",
      "     - Medium confidence (LR â‰¥ 0.70)\n",
      "     - LR predicts: TEXT\n",
      "\n",
      "2. last_name\n",
      "   Prediction: text\n",
      "   Confidence: 0.82\n",
      "   Evidence: 4 items\n",
      "     - No options in raw form\n",
      "     - Required field\n",
      "     - Medium confidence (LR â‰¥ 0.70)\n",
      "     - LR predicts: TEXT\n",
      "\n",
      "3. email\n",
      "   Prediction: text\n",
      "   Confidence: 0.85\n",
      "   Evidence: 4 items\n",
      "     - No options in raw form\n",
      "     - Required field\n",
      "     - Medium-high confidence (LR â‰¥ 0.85)\n",
      "     - LR predicts: TEXT\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING: predictions.jsonl\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    predictions_list = []\n",
    "    \n",
    "    for idx, row in phase3_results.iterrows():\n",
    "        # Generate evidence\n",
    "        evidence = generate_evidence(row)\n",
    "        \n",
    "        # Create prediction object\n",
    "        prediction = {\n",
    "            \"id\": str(row['field_id']),\n",
    "            \"label\": str(row['field_id']),  # Using field_id as label\n",
    "            \"input_kind_pred\": str(row['lr_prediction']).lower(),\n",
    "            \"confidence\": float(round(row['lr_confidence'], 2)),\n",
    "            \"evidence\": evidence\n",
    "        }\n",
    "        predictions_list.append(prediction)\n",
    "    \n",
    "    # Save as JSONL\n",
    "    with open('predictions.jsonl', 'w') as f:\n",
    "        for pred in predictions_list:\n",
    "            f.write(json.dumps(pred) + '\\n')\n",
    "    \n",
    "    print(f\"\\nâœ… Saved: predictions.jsonl\")\n",
    "    print(f\"   Lines: {len(predictions_list)}\")\n",
    "    print(f\"   Format: One JSON object per line\")\n",
    "    \n",
    "    # Show first 3 predictions\n",
    "    print(f\"\\nðŸ“‹ First 3 predictions:\")\n",
    "    for i, pred in enumerate(predictions_list[:3]):\n",
    "        print(f\"\\n{i+1}. {pred['id']}\")\n",
    "        print(f\"   Prediction: {pred['input_kind_pred']}\")\n",
    "        print(f\"   Confidence: {pred['confidence']}\")\n",
    "        print(f\"   Evidence: {len(pred['evidence'])} items\")\n",
    "        for ev in pred['evidence']:\n",
    "            print(f\"     - {ev}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Also Generate predictions.json (JSON Array Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GENERATING: predictions.json (JSON Array Format)\n",
      "======================================================================\n",
      "\n",
      "âœ… Saved: predictions.json\n",
      "   Format: JSON array\n",
      "   Fields: 23\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING: predictions.json (JSON Array Format)\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Save as JSON array\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(predictions_list, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nâœ… Saved: predictions.json\")\n",
    "    print(f\"   Format: JSON array\")\n",
    "    print(f\"   Fields: {len(predictions_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Verify Output Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUALITY VERIFICATION\n",
      "======================================================================\n",
      "\n",
      "âœ… Count Checks:\n",
      "   Total predictions: 23\n",
      "   Expected: 23\n",
      "   Match: True\n",
      "\n",
      "âœ… Format Checks:\n",
      "   âœ… All predictions have correct format\n",
      "\n",
      "âœ… Confidence Distribution:\n",
      "   Mean: 0.823\n",
      "   Min: 0.540\n",
      "   Max: 1.000\n",
      "\n",
      "âœ… Prediction Distribution:\n",
      "   TEXT: 14\n",
      "   SELECT: 9\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"QUALITY VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Verify counts\n",
    "    print(f\"\\nâœ… Count Checks:\")\n",
    "    print(f\"   Total predictions: {len(predictions_list)}\")\n",
    "    print(f\"   Expected: 23\")\n",
    "    print(f\"   Match: {len(predictions_list) == 23}\")\n",
    "    \n",
    "    # Verify format\n",
    "    print(f\"\\nâœ… Format Checks:\")\n",
    "    \n",
    "    all_valid = True\n",
    "    for i, pred in enumerate(predictions_list):\n",
    "        # Check required fields\n",
    "        if not all(k in pred for k in ['id', 'label', 'input_kind_pred', 'confidence', 'evidence']):\n",
    "            print(f\"   âŒ Prediction {i}: Missing required fields\")\n",
    "            all_valid = False\n",
    "        \n",
    "        # Check prediction value\n",
    "        if pred['input_kind_pred'] not in ['text', 'select']:\n",
    "            print(f\"   âŒ Prediction {i}: Invalid input_kind_pred: {pred['input_kind_pred']}\")\n",
    "            all_valid = False\n",
    "        \n",
    "        # Check confidence range\n",
    "        if not (0.0 <= pred['confidence'] <= 1.0):\n",
    "            print(f\"   âŒ Prediction {i}: Confidence out of range: {pred['confidence']}\")\n",
    "            all_valid = False\n",
    "        \n",
    "        # Check evidence\n",
    "        if not isinstance(pred['evidence'], list) or len(pred['evidence']) == 0:\n",
    "            print(f\"   âŒ Prediction {i}: Invalid evidence\")\n",
    "            all_valid = False\n",
    "    \n",
    "    if all_valid:\n",
    "        print(f\"   âœ… All predictions have correct format\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    confidences = [p['confidence'] for p in predictions_list]\n",
    "    print(f\"\\nâœ… Confidence Distribution:\")\n",
    "    print(f\"   Mean: {sum(confidences)/len(confidences):.3f}\")\n",
    "    print(f\"   Min: {min(confidences):.3f}\")\n",
    "    print(f\"   Max: {max(confidences):.3f}\")\n",
    "    \n",
    "    # Prediction distribution\n",
    "    text_count = sum(1 for p in predictions_list if p['input_kind_pred'] == 'text')\n",
    "    select_count = sum(1 for p in predictions_list if p['input_kind_pred'] == 'select')\n",
    "    print(f\"\\nâœ… Prediction Distribution:\")\n",
    "    print(f\"   TEXT: {text_count}\")\n",
    "    print(f\"   SELECT: {select_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL CHECKLIST\n",
      "======================================================================\n",
      "\n",
      "âœ… SUBMISSION REQUIREMENTS MET:\n",
      "  âœ… 23 field predictions (100% coverage)\n",
      "  âœ… Correct JSON format (id, label, input_kind_pred, confidence, evidence)\n",
      "  âœ… Predictions are 'text' or 'select' (lowercase)\n",
      "  âœ… Confidence scores 0.0-1.0\n",
      "  âœ… Evidence provided for each prediction\n",
      "  âœ… Valid JSON/JSONL format\n",
      "  \n",
      "ðŸ“ OUTPUT FILES CREATED:\n",
      "  âœ… predictions.jsonl (RECOMMENDED)\n",
      "     - One JSON object per line\n",
      "     - File size: 5.4 KB\n",
      "  \n",
      "  âœ… predictions.json (ALTERNATIVE)\n",
      "     - JSON array format\n",
      "     - File size: 6.9 KB\n",
      "\n",
      "ðŸŽ¯ APPROACH:\n",
      "  - Phase 1: Baseline (73.9%)\n",
      "  - Phase 2: Rule-Based (87.0%)\n",
      "  - Phase 3: Logistic Regression (87.0%) â† FINAL\n",
      "  - Phase 4: Skipped (DOM incomplete)\n",
      "  \n",
      "ðŸ“Š FINAL ACCURACY: 87.0%\n",
      "\n",
      "ðŸš€ READY TO SUBMIT!\n",
      "   Use: predictions.jsonl\n",
      "   \n",
      "Checklist:\n",
      "  âœ… Schema matches PDF requirement\n",
      "  âœ… All fields included\n",
      "  âœ… Evidence provided\n",
      "  âœ… Valid JSON format\n",
      "  âœ… Ready for submission\n",
      "    \n",
      "\n",
      "======================================================================\n",
      "âœ… SUBMISSION READY!\n",
      "======================================================================\n",
      "\n",
      "ðŸ“¤ Submit: predictions.jsonl\n",
      "\n",
      "File is in the current directory and ready to upload!\n"
     ]
    }
   ],
   "source": [
    "if phase3_results is not None:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FINAL CHECKLIST\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    checklist = f\"\"\"\n",
    "âœ… SUBMISSION REQUIREMENTS MET:\n",
    "  âœ… 23 field predictions (100% coverage)\n",
    "  âœ… Correct JSON format (id, label, input_kind_pred, confidence, evidence)\n",
    "  âœ… Predictions are 'text' or 'select' (lowercase)\n",
    "  âœ… Confidence scores 0.0-1.0\n",
    "  âœ… Evidence provided for each prediction\n",
    "  âœ… Valid JSON/JSONL format\n",
    "  \n",
    "ðŸ“ OUTPUT FILES CREATED:\n",
    "  âœ… predictions.jsonl (RECOMMENDED)\n",
    "     - One JSON object per line\n",
    "     - File size: {round(__import__('os').path.getsize('predictions.jsonl')/1024, 1)} KB\n",
    "  \n",
    "  âœ… predictions.json (ALTERNATIVE)\n",
    "     - JSON array format\n",
    "     - File size: {round(__import__('os').path.getsize('predictions.json')/1024, 1)} KB\n",
    "\n",
    "ðŸŽ¯ APPROACH:\n",
    "  - Phase 1: Baseline (73.9%)\n",
    "  - Phase 2: Rule-Based (87.0%)\n",
    "  - Phase 3: Logistic Regression (87.0%) â† FINAL\n",
    "  - Phase 4: Skipped (DOM incomplete)\n",
    "  \n",
    "ðŸ“Š FINAL ACCURACY: {(phase3_results['lr_correct'].mean()*100):.1f}%\n",
    "\n",
    "ðŸš€ READY TO SUBMIT!\n",
    "   Use: predictions.jsonl\n",
    "   \n",
    "Checklist:\n",
    "  âœ… Schema matches PDF requirement\n",
    "  âœ… All fields included\n",
    "  âœ… Evidence provided\n",
    "  âœ… Valid JSON format\n",
    "  âœ… Ready for submission\n",
    "    \"\"\"\n",
    "    \n",
    "    print(checklist)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*70)\n",
    "    print(f\"âœ… SUBMISSION READY!\")\n",
    "    print(f\"=\"*70)\n",
    "    print(f\"\\nðŸ“¤ Submit: predictions.jsonl\")\n",
    "    print(f\"\\nFile is in the current directory and ready to upload!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
